{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6adbdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppq.lib as PFL\n",
    "from ppq.api import ENABLE_CUDA_KERNEL, load_onnx_graph\n",
    "from ppq.core import TargetPlatform\n",
    "from ppq.executor import TorchExecutor\n",
    "from ppq.quantization.optim import (LayerwiseEqualizationPass,\n",
    "                                    LearnedStepSizePass, ParameterQuantizePass,\n",
    "                                    RuntimeCalibrationPass)\n",
    "\n",
    "import torch\n",
    "\n",
    "from Quantizers import MyInt8Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf92390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "from data import MyData\n",
    "\n",
    "BATCH_SIZE_DATA = 128\n",
    "DATA_PATH = \"../data/MNIST/\"\n",
    "onnxFile = \"./model.onnx\"\n",
    "ppq_onnxFile = \"./model_int8(PPQ).onnx\"\n",
    "int8_scale_file = \"./model_int8(PPQ).json\"\n",
    "engine_file = './model_int8(PPQ).engine'\n",
    "\n",
    "trainDataset = MyData(datapath = DATA_PATH, isTrain = True)\n",
    "\n",
    "calibLoader = torch.utils.data.DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE_DATA, shuffle=True, collate_fn = lambda x: torch.cat([sample[0].unsqueeze(0) for sample in x], dim=0))\n",
    "\n",
    "input_shape = next(iter(calibLoader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4a5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = load_onnx_graph(onnx_import_file=onnxFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "164c0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = PFL.Quantizer(platform=TargetPlatform.TRT_INT8, graph=graph)\n",
    "# quantizer = MyInt8Quantizer(graph=graph)\n",
    "\n",
    "for name, op in graph.operations.items():\n",
    "    if op.type in {'Conv', 'ConvTranspose', 'MatMul', 'Gemm', \n",
    "                   'PPQBiasFusedMatMul', 'LayerNormalization'}:\n",
    "        quantizer.quantize_operation(name, platform=TargetPlatform.TRT_INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "278a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PFL.Pipeline([\n",
    "            # LayerwiseEqualizationPass(iteration=10),\n",
    "            ParameterQuantizePass(),\n",
    "            RuntimeCalibrationPass(),\n",
    "            # LearnedStepSizePass(\n",
    "            #     steps=1000, is_scale_trainable=False, \n",
    "            #     lr=1e-4, block_size=4, collecting_device='cpu'),\n",
    "            # ParameterBakingPass()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce51f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # call pipeline.\n",
    "executor = TorchExecutor(graph=graph)\n",
    "executor.tracing_operation_meta(torch.zeros(input_shape).cuda())\n",
    "executor.load_graph(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c88dae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:34:10] PPQ Parameter Quantization Pass Running ... Finished.\n",
      "[02:34:11] PPQ Runtime Calibration Pass Running ...    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1): 100%|████████████████████████| 100/100 [00:06<00:00, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.optimize(\n",
    "    graph=graph, dataloader=calibLoader, verbose=True,\n",
    "    calib_steps=100, collate_fn=lambda x: x.to('cuda'), executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11296f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[Info] You are exporting PPQ Graph to TensorRT(Onnx + Json). \n",
      "Please Compile the TensorRT INT8 engine manually: \n",
      "\n",
      "from ppq.utils.TensorRTUtil import build_engine \n",
      "build_engine(onnx_file='Quantized.onnx', int8_scale_file='Quantized.json', engine_file='Quantized.engine', int8=True)\n",
      "\u001b[0m\n",
      "\u001b[33m[Info] Parameters have been saved to file: ./quantized.wts\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "exporter = PFL.Exporter(platform=TargetPlatform.TRT_INT8)\n",
    "exporter.export(file_path=ppq_onnxFile, graph=graph, config_path=int8_scale_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a22e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/31/2023-02:34:52] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[05/31/2023-02:34:52] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[05/31/2023-02:34:52] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor /Relu_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor /Relu_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor /MaxPool_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Matrix Multiply]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [Matrix Multiply]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 18) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 19) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor /Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 21) [TopK]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 21) [TopK]_output_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:34:52] [TRT] [W] Missing scale and zero-point for tensor z, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-02:35:10] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[05/31/2023-02:35:10] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[05/31/2023-02:35:10] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[05/31/2023-02:35:10] [TRT] [W] - 5 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[05/31/2023-02:35:10] [TRT] [W] - 2 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "from ppq.utils.TensorRTUtil import build_engine \n",
    "build_engine(onnx_file=ppq_onnxFile, int8_scale_file=int8_scale_file, engine_file=engine_file, int8=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23360125",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f33503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
