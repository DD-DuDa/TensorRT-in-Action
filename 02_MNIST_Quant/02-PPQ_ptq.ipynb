{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e444c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ppq.lib as PFL\n",
    "from ppq.api import ENABLE_CUDA_KERNEL, load_onnx_graph\n",
    "from ppq.core import TargetPlatform\n",
    "from ppq.executor import TorchExecutor\n",
    "from ppq.quantization.optim import (LayerwiseEqualizationPass,\n",
    "                                    LearnedStepSizePass, ParameterQuantizePass,\n",
    "                                    RuntimeCalibrationPass)\n",
    "\n",
    "import torch\n",
    "\n",
    "from Quantizers import MyInt8Quantizer\n",
    "\n",
    "import tensorrt as trt\n",
    "from typing import Optional, List, Tuple\n",
    "from cuda import cuda, cudart\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888c54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "from data import MyData\n",
    "\n",
    "BATCH_SIZE_DATA = 128\n",
    "DATA_PATH = \"../data/MNIST/\"\n",
    "\n",
    "onnxFile = \"./model.onnx\"\n",
    "ppq_onnxFile = \"./model_int8(PPQ)_onnx.onnx\"  # 只针对参数量化，表示参数已经被静态量化，当前 config 不生效，数据可以直接使用\n",
    "int8_scale_file = \"./model_int8(PPQ).json\"\n",
    "engine_file = './model_int8(PPQ).engine'\n",
    "\n",
    "trainDataset = MyData(datapath = DATA_PATH, isTrain = True)\n",
    "\n",
    "calibLoader = torch.utils.data.DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE_DATA, shuffle=True, collate_fn = lambda x: torch.cat([sample[0].unsqueeze(0) for sample in x], dim=0))\n",
    "\n",
    "input_shape = next(iter(calibLoader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f111f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = load_onnx_graph(onnx_import_file=onnxFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f16689",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = PFL.Quantizer(platform=TargetPlatform.TRT_INT8, graph=graph)\n",
    "# quantizer = MyInt8Quantizer(graph=graph)\n",
    "\n",
    "for name, op in graph.operations.items():\n",
    "    if op.type in {'Conv', 'ConvTranspose', 'MatMul', 'Gemm', \n",
    "                   'PPQBiasFusedMatMul', 'LayerNormalization'}:\n",
    "        quantizer.quantize_operation(name, platform=TargetPlatform.TRT_INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16638b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PFL.Pipeline([\n",
    "            # LayerwiseEqualizationPass(iteration=10),\n",
    "            ParameterQuantizePass(),\n",
    "            RuntimeCalibrationPass(),\n",
    "            # LearnedStepSizePass(\n",
    "            #     steps=1000, is_scale_trainable=False, \n",
    "            #     lr=1e-4, block_size=4, collecting_device='cpu'),\n",
    "            # ParameterBakingPass()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d251d400",
   "metadata": {},
   "outputs": [],
   "source": [
    " # call pipeline.\n",
    "executor = TorchExecutor(graph=graph)\n",
    "executor.tracing_operation_meta(torch.zeros(input_shape).cuda())\n",
    "executor.load_graph(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfac5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:50:27] PPQ Parameter Quantization Pass Running ... Finished.\n",
      "[09:50:27] PPQ Runtime Calibration Pass Running ...    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1): 100%|████████████████████████████| 8/8 [00:00<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.optimize(\n",
    "    graph=graph, dataloader=calibLoader, verbose=True,\n",
    "    calib_steps=8, collate_fn=lambda x: x.to('cuda'), executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0258e86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[Info] You are exporting PPQ Graph to TensorRT(Onnx + Json). \n",
      "Please Compile the TensorRT INT8 engine manually: \n",
      "\n",
      "from ppq.utils.TensorRTUtil import build_engine \n",
      "build_engine(onnx_file='Quantized.onnx', int8_scale_file='Quantized.json', engine_file='Quantized.engine', int8=True)\n",
      "\u001b[0m\n",
      "\u001b[33m[Info] Parameters have been saved to file: ./quantized.wts\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "exporter = PFL.Exporter(platform=TargetPlatform.TRT_INT8)\n",
    "exporter.export(file_path=ppq_onnxFile, graph=graph, config_path=int8_scale_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ac77a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/31/2023-09:50:35] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[05/31/2023-09:50:35] [TRT] [W] Tensor DataType is determined at build time for tensors not marked as input or output.\n",
      "[05/31/2023-09:50:35] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor /Relu_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor /Relu_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor /MaxPool_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 7) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 8) [Matrix Multiply]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 10) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 13) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 14) [Matrix Multiply]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 15) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 16) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 18) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 19) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor /Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 21) [TopK]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 21) [TopK]_output_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:35] [TRT] [W] Missing scale and zero-point for tensor z, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[05/31/2023-09:50:52] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[05/31/2023-09:50:52] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[05/31/2023-09:50:52] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[05/31/2023-09:50:52] [TRT] [W] - 5 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[05/31/2023-09:50:52] [TRT] [W] - 2 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "from ppq.utils.TensorRTUtil import build_engine \n",
    "build_engine(onnx_file=ppq_onnxFile, int8_scale_file=int8_scale_file, engine_file=engine_file, int8=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e44d8",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85a7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DATA_PATH = \"../data/MNIST/\"\n",
    "\n",
    "testDataset = MyData(datapath = DATA_PATH, isTrain = False)\n",
    "testLoader = torch.utils.data.DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730c45ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id: 0    is input:  TensorIOMode.INPUT   binding name: x   shape: (128, 1, 28, 28) type:  DataType.FLOAT\n",
      "input id: 1    is input:  TensorIOMode.OUTPUT   binding name: y   shape: (128, 10) type:  DataType.FLOAT\n",
      "input id: 2    is input:  TensorIOMode.OUTPUT   binding name: z   shape: (128,) type:  DataType.INT32\n",
      "\n",
      "Test set: Accuracy: 95.700%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2064/1354393900.py:18: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(i, (128, 1, 28, 28))\n"
     ]
    }
   ],
   "source": [
    "logger = trt.Logger(trt.Logger.ERROR) \n",
    "\n",
    "with open(engine_file, \"rb\") as f, trt.Runtime(logger) as runtime, runtime.deserialize_cuda_engine(\n",
    "    f.read()\n",
    ") as engine, engine.create_execution_context() as context:\n",
    "    input_len = 0\n",
    "    correct = 0\n",
    "    for idx in range(engine.num_bindings):\n",
    "        name = engine.get_tensor_name (idx)\n",
    "        is_input = engine.get_tensor_mode (name)\n",
    "        if is_input == trt.TensorIOMode.INPUT:\n",
    "            input_len += 1\n",
    "        op_type = engine.get_tensor_dtype(name)\n",
    "        shape = engine.get_tensor_shape(name)\n",
    "        print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)\n",
    "        \n",
    "    for i in range(input_len):\n",
    "        context.set_binding_shape(i, (128, 1, 28, 28))\n",
    "    assert context.all_binding_shapes_specified\n",
    "    \n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine, context, 0)\n",
    "    \n",
    "    for _, (image, label) in enumerate(testLoader):\n",
    "        image = image.numpy()\n",
    "        inputs[0].host = image\n",
    "        preds = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)[0][1]\n",
    "        label = np.argmax(label.numpy(), axis=-1)\n",
    "        count = 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == preds[i]:\n",
    "                count += 1\n",
    "        correct += count\n",
    "    print('\\nTest set: Accuracy: {:.3f}%\\n'.format(100. * correct / len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a6759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22c736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
