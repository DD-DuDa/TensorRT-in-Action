{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc83a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ppq.lib as PFL\n",
    "from ppq.api import ENABLE_CUDA_KERNEL, load_onnx_graph\n",
    "from ppq.core import TargetPlatform\n",
    "from ppq.executor import TorchExecutor\n",
    "from ppq.quantization.optim import (LayerwiseEqualizationPass,\n",
    "                                    LearnedStepSizePass, ParameterQuantizePass,\n",
    "                                    RuntimeCalibrationPass)\n",
    "from ppq.quantization.quantizer import TensorRTQuantizer\n",
    "from ppq.core import (ChannelwiseTensorQuantizationConfig, OperationMeta,\n",
    "                      OperationQuantizationConfig, QuantizationPolicy,\n",
    "                      QuantizationProperty, QuantizationStates, RoundingPolicy,\n",
    "                      TargetPlatform)\n",
    "from ppq.IR import BaseGraph\n",
    "\n",
    "import torch\n",
    "\n",
    "import tensorrt as trt\n",
    "from typing import Optional, List, Tuple\n",
    "from cuda import cuda, cudart\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from squad_dataset import get_squad_dataset, post_processing_function, postprocess_qa_predictions\n",
    "from transformers import default_data_collator, EvalPrediction\n",
    "from transformers.trainer_pt_utils import nested_concat, nested_truncate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Union\n",
    "import common\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "from squad_dataset import get_squad_dataset, post_processing_function, postprocess_qa_predictions\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5805bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "onnxFile = \"./onnx/distilbert-squad.onnx\"\n",
    "ppq_onnxFile = \"./onnx/distilbert-squad_int8(PPQ).onnx\"\n",
    "int8_scale_file = \"./distilbert-squad_int8(PPQ).json\"\n",
    "engine_file = './engine/distilbert-squad_int8(PPQ).engine'\n",
    "\n",
    "min_batch_size = 1\n",
    "norm_batch_size = 16\n",
    "max_batch_size = 64\n",
    "\n",
    "max_length = 384 # 输入数据的最大长度\n",
    "doc_stride = 128 # 当切分时，重叠的长度\n",
    "\n",
    "norm_shape = (norm_batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0222a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sun May 28 02:10:10 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.71it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-07fafce74b00717e.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_examples, eval_dataset = get_squad_dataset(model_checkpoint, for_model = False)\n",
    "eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset_for_model, collate_fn=data_collator, batch_size=norm_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b4a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:11:15] PPQ Parameter Quantization Pass Running ... Finished.\n",
      "[04:11:15] PPQ Runtime Calibration Pass Running ...    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1):  56%|██████████████████████████▍                    | 9/16 [01:44<01:22, 11.78s/it]"
     ]
    }
   ],
   "source": [
    "graph = load_onnx_graph(onnx_import_file=onnxFile)\n",
    "\n",
    "quantizer   = PFL.Quantizer(platform=TargetPlatform.TRT_INT8, graph=graph)\n",
    "  \n",
    "for name, op in graph.operations.items():\n",
    "    if op.type in {'Conv', 'ConvTranspose', 'MatMul', 'Gemm', \n",
    "                   'PPQBiasFusedMatMul', 'LayerNormalization'}:\n",
    "        quantizer.quantize_operation(name, platform=TargetPlatform.TRT_INT8)\n",
    "    \n",
    "pipeline = PFL.Pipeline([\n",
    "            ParameterQuantizePass(),\n",
    "            RuntimeCalibrationPass(),\n",
    "            ])\n",
    "\n",
    " # call pipeline.\n",
    "executor = TorchExecutor(graph=graph)\n",
    "executor.tracing_operation_meta([torch.zeros(norm_shape, dtype=torch.int32).cuda(),torch.zeros(norm_shape, dtype=torch.int32).cuda()])\n",
    "executor.load_graph(graph=graph)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    collated_batch = {}\n",
    "    for key in batch:\n",
    "        collated_batch[key] = batch[key].to('cuda')\n",
    "    return collated_batch\n",
    "\n",
    "pipeline.optimize(\n",
    "    graph=graph, dataloader=eval_dataloader, verbose=True,\n",
    "    calib_steps=16, collate_fn=collate_fn, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb89b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exporter = PFL.Exporter(platform=TargetPlatform.TRT_INT8)\n",
    "exporter.export(file_path=ppq_onnxFile, graph=graph, config_path=int8_scale_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06766839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ppq.utils.TensorRTUtil import build_engine \n",
    "build_engine(onnx_file=ppq_onnxFile, int8_scale_file=int8_scale_file, engine_file=engine_file, int8=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = trt.Logger(trt.Logger.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(engine_file, \"rb\") as f, trt.Runtime(logger) as runtime, runtime.deserialize_cuda_engine(\n",
    "    f.read()\n",
    ") as engine, engine.create_execution_context() as context:\n",
    "    input_len = 0\n",
    "    print(\"***** Engine IO *****\")\n",
    "    for idx in range(engine.num_bindings):\n",
    "        name = engine.get_tensor_name (idx)\n",
    "        is_input = engine.get_tensor_mode (name)\n",
    "        if is_input == trt.TensorIOMode.INPUT:\n",
    "            input_len += 1\n",
    "        op_type = engine.get_tensor_dtype(name)\n",
    "        shape = engine.get_tensor_shape(name)\n",
    "        print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)\n",
    "    print(\"*****           *****\")\n",
    "        \n",
    "    for i in range(input_len):\n",
    "        context.set_binding_shape(i, (norm_batch_size, max_length))\n",
    "    assert context.all_binding_shapes_specified\n",
    "    \n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine, context, 0)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"***** Running Evaluation *****\")\n",
    "    print(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    print(f\"  Batch size = {norm_batch_size}\")\n",
    "\n",
    "    total_time = 0.0\n",
    "    niter = 0\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    all_preds = None\n",
    "        \n",
    "    for step, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        input_ids = np.asarray(batch[\"input_ids\"], dtype=np.int32)\n",
    "        attention_mask = np.asarray(batch[\"attention_mask\"], dtype=np.int32)\n",
    "\n",
    "        inputs[0].host = input_ids.ravel()\n",
    "        inputs[1].host = attention_mask.ravel()\n",
    "        \n",
    "        trt_outputs, infer_time = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "\n",
    "        start_logits, end_logits = trt_outputs\n",
    "        start_logits = torch.tensor(start_logits).reshape(norm_batch_size, max_length)\n",
    "        end_logits = torch.tensor(end_logits).reshape(norm_batch_size, max_length)\n",
    "        \n",
    "        total_time += infer_time\n",
    "        niter += 1\n",
    "\n",
    "        # necessary to pad predictions and labels for being gathered\n",
    "        start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "        end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        logits = (accelerator.gather(start_logits).cpu().numpy(), accelerator.gather(end_logits).cpu().numpy())\n",
    "        all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "\n",
    "    if all_preds is not None:\n",
    "        all_preds = nested_truncate(all_preds, len(eval_dataset))\n",
    "        \n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    print(f\"Evaluation done in total {evalTime:.3f} secs ({evalTime / len(eval_dataset):.3f} sec per example)\")\n",
    "    # Inference time from TRT\n",
    "    print(\"Average Inference Time = {:.3f} ms\".format(total_time * 1000 / niter))\n",
    "    print(\"Total Inference Time =  {:.3f} ms\".format(total_time * 1000))\n",
    "    print(f\"Total Number of Inference =  {niter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99eaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039979c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, all_preds)\n",
    "\n",
    "eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "print(f\"Evaluation metrics: {eval_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8316893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
