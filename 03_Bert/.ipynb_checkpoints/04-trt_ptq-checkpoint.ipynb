{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0bdc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "from typing import Optional, List, Tuple\n",
    "from cuda import cuda, cudart\n",
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator, EvalPrediction\n",
    "from transformers.trainer_pt_utils import nested_concat, nested_truncate\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "# os.remove('./int8.cache')\n",
    "import common\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from squad_dataset import get_squad_dataset, post_processing_function, postprocess_qa_predictions\n",
    "\n",
    "accelerator = Accelerator()\n",
    "logger = trt.Logger(trt.Logger.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0253e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "onnx_model_path = \"./onnx/distilbert-squad.onnx\"\n",
    "engine_name = \"./engine/distilbert-int8.engine\"\n",
    "cacheFile = \"./int8.cache\"\n",
    "\n",
    "min_batch_size = 1\n",
    "norm_batch_size = 16\n",
    "max_batch_size = 64\n",
    "\n",
    "max_length = 384 # 输入数据的最大长度\n",
    "doc_stride = 128 # 当切分时，重叠的长度\n",
    "\n",
    "norm_shape = (norm_batch_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0a0f7",
   "metadata": {},
   "source": [
    "## 创建 Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a427b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sun May 28 02:10:10 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 465.00it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-07fafce74b00717e.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_examples, eval_dataset = get_squad_dataset(model_checkpoint, for_model = False)\n",
    "eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset_for_model, collate_fn=data_collator, batch_size=norm_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a9b247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SquadCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, dataset, input_shape, cache_file, num_calib = 100):\n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
    "        self.num_calib = num_calib\n",
    "        self.dataset = dataset[:num_calib]\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = input_shape[0]\n",
    "        self.cacheFile = cache_file\n",
    "        \n",
    "        self.buffer_size = trt.volume(self.input_shape) * trt.float32.itemsize\n",
    "        _, self.device_input_ids = cudart.cudaMalloc(self.buffer_size)\n",
    "        _, self.device_attention_mask = cudart.cudaMalloc(self.buffer_size)\n",
    "        \n",
    "        self.batches = self.batchGenerator()\n",
    "    \n",
    "    def __del__(self):\n",
    "        cudart.cudaFree(self.device_input_ids)\n",
    "        cudart.cudaFree(self.device_attention_mask)\n",
    "        \n",
    "    def batchGenerator(self):\n",
    "        for i in range(0, self.num_calib, self.batch_size):\n",
    "            input_ids_batch = self.dataset[\"input_ids\"][i: i + self.batch_size]\n",
    "            attention_mask_batch = self.dataset[\"attention_mask\"][i: i + self.batch_size]\n",
    "            input_ids = np.asarray(input_ids_batch, dtype=np.int32)\n",
    "            attention_mask = np.asarray(attention_mask_batch, dtype=np.int32)\n",
    "            yield input_ids.ravel(), attention_mask.ravel()\n",
    "            \n",
    "    def get_batch_size(self):  # necessary API\n",
    "        return self.batch_size  \n",
    "    \n",
    "    def get_batch(self, names=[], inputNodeName=None): # necessary API\n",
    "        try:\n",
    "            input_ids, attention_mask= next(self.batches)\n",
    "            cudart.cudaMemcpy(\n",
    "                self.device_input_ids, \n",
    "                input_ids.ctypes.data, \n",
    "                self.buffer_size, \n",
    "                cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n",
    "            )\n",
    "            cudart.cudaMemcpy(\n",
    "                self.device_attention_mask, \n",
    "                attention_mask.ctypes.data, \n",
    "                self.buffer_size, \n",
    "                cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n",
    "            )\n",
    "            return [int(self.device_input_ids), int(self.device_attention_mask)]\n",
    "        except StopIteration:\n",
    "            return None \n",
    "        \n",
    "    # 其他方法参照MyCalibrator\n",
    "    def read_calibration_cache(self):  # necessary API\n",
    "        if os.path.exists(self.cacheFile):\n",
    "            print(\"Succeed finding cahce file: %s\" % (self.cacheFile))\n",
    "            with open(self.cacheFile, \"rb\") as f:\n",
    "                cache = f.read()\n",
    "                return cache\n",
    "        else:\n",
    "            print(\"Failed finding int8 cache!\")\n",
    "            return\n",
    "\n",
    "    def write_calibration_cache(self, cache):  # necessary API\n",
    "        with open(self.cacheFile, \"wb\") as f:\n",
    "            f.write(cache)\n",
    "        print(\"Succeed saving int8 cache!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc52d9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/01/2023-03:51:03] [TRT] [I] [MemUsageChange] Init CUDA: CPU +11, GPU +0, now: CPU 142, GPU 262 (MiB)\n",
      "[06/01/2023-03:51:09] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1225, GPU +268, now: CPU 1443, GPU 530 (MiB)\n",
      "[06/01/2023-03:51:09] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "Succeeded parsing .onnx file!\n",
      "[06/01/2023-03:51:09] [TRT] [I] Graph optimization time: 0.0544468 seconds.\n",
      "Failed finding int8 cache!\n",
      "[06/01/2023-03:51:09] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[06/01/2023-03:51:09] [TRT] [W] Calibration Profile is not defined. Calibrating with Profile 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1634/704597152.py:24: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 1 << 50\n",
      "/tmp/ipykernel_1634/704597152.py:36: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/01/2023-03:51:12] [TRT] [I] Detected 2 inputs and 2 output network tensors.\n",
      "[06/01/2023-03:51:15] [TRT] [I] Total Host Persistent Memory: 368928\n",
      "[06/01/2023-03:51:15] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/01/2023-03:51:15] [TRT] [I] Total Scratch Memory: 0\n",
      "[06/01/2023-03:51:15] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 256 MiB\n",
      "[06/01/2023-03:51:15] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 468 steps to complete.\n",
      "[06/01/2023-03:51:15] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 48.7988ms to assign 28 blocks to 468 nodes requiring 356303872 bytes.\n",
      "[06/01/2023-03:51:15] [TRT] [I] Total Activation Memory: 356303872\n",
      "[06/01/2023-03:51:15] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +339, now: CPU 0, GPU 593 (MiB)\n",
      "[06/01/2023-03:51:15] [TRT] [I] Starting Calibration.\n",
      "[06/01/2023-03:51:15] [TRT] [I]   Calibrated batch 0 in 0.268789 seconds.\n",
      "[06/01/2023-03:51:16] [TRT] [I]   Calibrated batch 1 in 0.251111 seconds.\n",
      "[06/01/2023-03:51:16] [TRT] [I]   Calibrated batch 2 in 0.253455 seconds.\n",
      "[06/01/2023-03:51:16] [TRT] [I]   Calibrated batch 3 in 0.255604 seconds.\n",
      "[06/01/2023-03:51:16] [TRT] [I]   Calibrated batch 4 in 0.257137 seconds.\n",
      "[06/01/2023-03:51:17] [TRT] [I]   Calibrated batch 5 in 0.255394 seconds.\n",
      "[06/01/2023-03:51:17] [TRT] [I]   Calibrated batch 6 in 0.254215 seconds.\n",
      "[06/01/2023-03:52:17] [TRT] [I]   Post Processing Calibration data in 60.0344 seconds.\n",
      "[06/01/2023-03:52:17] [TRT] [I] Calibration completed in 67.4769 seconds.\n",
      "Failed finding int8 cache!\n",
      "[06/01/2023-03:52:17] [TRT] [I] Writing Calibration Cache for calibrator: TRT-8601-EntropyCalibration2\n",
      "Succeed saving int8 cache!\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor attention_mask, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 0) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 6) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/Slice_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 23) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 28) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 87) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 92) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 93) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Equal_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 113) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 139) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 160) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 164) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 180) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 235) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 258) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 259) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 280) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 284) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 301) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 305) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 309) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 321) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 325) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 380) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 403) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 404) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 425) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 450) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 470) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 525) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 548) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 549) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 570) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 574) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 591) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 595) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 599) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 611) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 615) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 670) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 693) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 694) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 715) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 719) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 736) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 740) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 744) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 756) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 760) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 815) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 838) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 839) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Softmax_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 860) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 864) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 881) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 885) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 889) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 901) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:17] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 905) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-03:52:18] [TRT] [I] Graph optimization time: 0.567121 seconds.\n",
      "[06/01/2023-03:52:18] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/01/2023-03:53:31] [TRT] [I] Detected 2 inputs and 2 output network tensors.\n",
      "[06/01/2023-03:53:31] [TRT] [I] Total Host Persistent Memory: 123072\n",
      "[06/01/2023-03:53:31] [TRT] [I] Total Device Persistent Memory: 1024\n",
      "[06/01/2023-03:53:31] [TRT] [I] Total Scratch Memory: 1132462080\n",
      "[06/01/2023-03:53:31] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 207 MiB, GPU 2100 MiB\n",
      "[06/01/2023-03:53:31] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 205 steps to complete.\n",
      "[06/01/2023-03:53:31] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 4.4194ms to assign 10 blocks to 205 nodes requiring 1509975552 bytes.\n",
      "[06/01/2023-03:53:31] [TRT] [I] Total Activation Memory: 1509974016\n",
      "[06/01/2023-03:53:32] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +41, GPU +133, now: CPU 41, GPU 133 (MiB)\n"
     ]
    }
   ],
   "source": [
    "logger = trt.Logger(trt.Logger.INFO) \n",
    "builder = trt.Builder(logger)                                           # create Builder\n",
    "config = builder.create_builder_config()                                # create BuidlerConfig to set meta data of the network\n",
    "\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "config.int8_calibrator = SquadCalibrator(eval_dataset, norm_shape, cacheFile)\n",
    "\n",
    "# 创建 Network 使用 Explicit Batch 模式，所有的维度都是显式的并且是动态的，意思是在执行的时候，每一维度的长度都可以变化\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "\n",
    "# 对 ONNX 进行模型解析\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "with open(onnx_model_path, \"rb\") as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        print(\"Failed parsing .onnx file!\")\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "        exit()\n",
    "    print(\"Succeeded parsing .onnx file!\")\n",
    "    \n",
    "network_inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "input_names = [_input.name for _input in network_inputs]  # ex: ['input_ids', 'attention_mask']\n",
    "\n",
    "config.max_workspace_size = 1 << 50\n",
    "\n",
    "# 由于使用 dynamic shape 需要 profile 指定输入范围，并让 profile 优化不同 shape 对应不同的 kernel\n",
    "profile = builder.create_optimization_profile()\n",
    "\n",
    "# 设置优化配置文件中输入张量的形状，包括最小、最优和最大形状\n",
    "for i in range(len(input_names)):\n",
    "    profile.set_shape(input_names[i], (min_batch_size, max_length), (norm_batch_size, max_length), (max_batch_size, max_length))\n",
    "    \n",
    "# 将优化配置文件添加到TensorRT配置中\n",
    "config.add_optimization_profile(profile) \n",
    "\n",
    "engine = builder.build_engine(network, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552d5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(engine_name, \"wb\") as f:\n",
    "    f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066fd39",
   "metadata": {},
   "source": [
    "## TRT 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487090b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/01/2023-03:55:21] [TRT] [I] Loaded engine size: 136 MiB\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +132, now: CPU 0, GPU 132 (MiB)\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MS] Running engine with multi stream info\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MS] Number of aux streams is 2\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MS] Number of total worker streams is 3\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[06/01/2023-03:55:21] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1440, now: CPU 0, GPU 1572 (MiB)\n",
      "input id: 0    is input:  TensorIOMode.INPUT   binding name: input_ids   shape: (-1, 384) type:  DataType.INT32\n",
      "input id: 1    is input:  TensorIOMode.INPUT   binding name: attention_mask   shape: (-1, 384) type:  DataType.INT32\n",
      "input id: 2    is input:  TensorIOMode.OUTPUT   binding name: output_start_logits   shape: (-1, 384) type:  DataType.FLOAT\n",
      "input id: 3    is input:  TensorIOMode.OUTPUT   binding name: output_end_logits   shape: (-1, 384) type:  DataType.FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1783/4099779251.py:15: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(i, (norm_batch_size, max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "674it [00:16, 41.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation done in total 16.137154850177467 secs (0.0014963978904096317 sec per example)\n",
      "Average Inference Time = 12.565 ms\n",
      "Total Inference Time =  8468.917 ms\n",
      "Total Number of Inference =  %d 674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(engine_name, \"rb\") as f, trt.Runtime(logger) as runtime, runtime.deserialize_cuda_engine(\n",
    "    f.read()\n",
    ") as engine, engine.create_execution_context() as context:\n",
    "    input_len = 0\n",
    "    print(\"***** Engine IO *****\")\n",
    "    for idx in range(engine.num_bindings):\n",
    "        name = engine.get_tensor_name (idx)\n",
    "        is_input = engine.get_tensor_mode (name)\n",
    "        if is_input == trt.TensorIOMode.INPUT:\n",
    "            input_len += 1\n",
    "        op_type = engine.get_tensor_dtype(name)\n",
    "        shape = engine.get_tensor_shape(name)\n",
    "        print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)\n",
    "    print(\"*****           *****\")\n",
    "        \n",
    "    for i in range(input_len):\n",
    "        context.set_binding_shape(i, (norm_batch_size, max_length))\n",
    "    assert context.all_binding_shapes_specified\n",
    "    \n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine, context, 0)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"***** Running Evaluation *****\")\n",
    "    print(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    print(f\"  Batch size = {norm_batch_size}\")\n",
    "\n",
    "    total_time = 0.0\n",
    "    niter = 0\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    all_preds = None\n",
    "        \n",
    "    for step, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        input_ids = np.asarray(batch[\"input_ids\"], dtype=np.int32)\n",
    "        attention_mask = np.asarray(batch[\"attention_mask\"], dtype=np.int32)\n",
    "\n",
    "        inputs[0].host = input_ids.ravel()\n",
    "        inputs[1].host = attention_mask.ravel()\n",
    "        \n",
    "        trt_outputs, infer_time = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "\n",
    "        start_logits, end_logits = trt_outputs\n",
    "        start_logits = torch.tensor(start_logits).reshape(norm_batch_size, max_length)\n",
    "        end_logits = torch.tensor(end_logits).reshape(norm_batch_size, max_length)\n",
    "        \n",
    "        total_time += infer_time\n",
    "        niter += 1\n",
    "\n",
    "        # necessary to pad predictions and labels for being gathered\n",
    "        start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "        end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        logits = (accelerator.gather(start_logits).cpu().numpy(), accelerator.gather(end_logits).cpu().numpy())\n",
    "        all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "\n",
    "    if all_preds is not None:\n",
    "        all_preds = nested_truncate(all_preds, len(eval_dataset))\n",
    "        \n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    print(f\"Evaluation done in total {evalTime:.3f} secs ({evalTime / len(eval_dataset):.3f} sec per example)\")\n",
    "    # Inference time from TRT\n",
    "    print(\"Average Inference Time = {:.3f} ms\".format(total_time * 1000 / niter))\n",
    "    print(\"Total Inference Time =  {:.3f} ms\".format(total_time * 1000))\n",
    "    print(f\"Total Number of Inference =  {niter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11b645",
   "metadata": {},
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a486433d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 10570/10570 [00:26<00:00, 406.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'exact_match': 67.43614001892148, 'f1': 78.40160484497551}\n"
     ]
    }
   ],
   "source": [
    "squad_v2 = False\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, all_preds)\n",
    "\n",
    "eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "print(f\"Evaluation metrics: {eval_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b6925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
