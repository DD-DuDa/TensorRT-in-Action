{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893ee75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      ____  ____  __   ____                    __              __\n",
      "     / __ \\/ __ \\/ /  / __ \\__  ______ _____  / /_____  ____  / /\n",
      "    / /_/ / /_/ / /  / / / / / / / __ `/ __ \\/ __/ __ \\/ __ \\/ /\n",
      "   / ____/ ____/ /__/ /_/ / /_/ / /_/ / / / / /_/ /_/ / /_/ / /\n",
      "  /_/   /_/   /_____\\___\\_\\__,_/\\__,_/_/ /_/\\__/\\____/\\____/_/\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ppq.lib as PFL\n",
    "from ppq.api import ENABLE_CUDA_KERNEL, load_onnx_graph\n",
    "from ppq.core import TargetPlatform\n",
    "from ppq.executor import TorchExecutor\n",
    "from ppq.quantization.optim import (LayerwiseEqualizationPass,\n",
    "                                    LearnedStepSizePass, ParameterQuantizePass,\n",
    "                                    RuntimeCalibrationPass)\n",
    "from ppq.quantization.quantizer import TensorRTQuantizer\n",
    "from ppq.core import (ChannelwiseTensorQuantizationConfig, OperationMeta,\n",
    "                      OperationQuantizationConfig, QuantizationPolicy,\n",
    "                      QuantizationProperty, QuantizationStates, RoundingPolicy,\n",
    "                      TargetPlatform)\n",
    "from ppq.IR import BaseGraph\n",
    "\n",
    "import torch\n",
    "\n",
    "import tensorrt as trt\n",
    "from typing import Optional, List, Tuple\n",
    "from cuda import cuda, cudart\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from squad_dataset import get_squad_dataset, post_processing_function, postprocess_qa_predictions\n",
    "from transformers import default_data_collator, EvalPrediction\n",
    "from transformers.trainer_pt_utils import nested_concat, nested_truncate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Union\n",
    "import common\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "from squad_dataset import get_squad_dataset, post_processing_function, postprocess_qa_predictions\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec02d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "onnxFile = \"./onnx/distilbert-squad.onnx\"\n",
    "ppq_onnxFile = \"./onnx/distilbert-squad_int8(PPQ).onnx\"\n",
    "int8_scale_file = \"./distilbert-squad_int8(PPQ).json\"\n",
    "engine_file = './engine/distilbert-squad_int8(PPQ).engine'\n",
    "\n",
    "min_batch_size = 1\n",
    "norm_batch_size = 16\n",
    "max_batch_size = 64\n",
    "\n",
    "max_length = 384 # 输入数据的最大长度\n",
    "doc_stride = 128 # 当切分时，重叠的长度\n",
    "\n",
    "norm_shape = (norm_batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24da8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Sun May 28 02:10:10 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 427.71it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-07fafce74b00717e.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_examples, eval_dataset = get_squad_dataset(model_checkpoint, for_model = False)\n",
    "eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset_for_model, collate_fn=data_collator, batch_size=norm_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19eb35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:11:15] PPQ Parameter Quantization Pass Running ... Finished.\n",
      "[04:11:15] PPQ Runtime Calibration Pass Running ...    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration Progress(Phase 1): 100%|██████████████████████████████████████████████| 16/16 [03:07<00:00, 11.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "graph = load_onnx_graph(onnx_import_file=onnxFile)\n",
    "\n",
    "quantizer   = PFL.Quantizer(platform=TargetPlatform.TRT_INT8, graph=graph)\n",
    "  \n",
    "for name, op in graph.operations.items():\n",
    "    if op.type in {'Conv', 'ConvTranspose', 'MatMul', 'Gemm', \n",
    "                   'PPQBiasFusedMatMul', 'LayerNormalization'}:\n",
    "        quantizer.quantize_operation(name, platform=TargetPlatform.TRT_INT8)\n",
    "    \n",
    "pipeline = PFL.Pipeline([\n",
    "            ParameterQuantizePass(),\n",
    "            RuntimeCalibrationPass(),\n",
    "            ])\n",
    "\n",
    " # call pipeline.\n",
    "executor = TorchExecutor(graph=graph)\n",
    "executor.tracing_operation_meta([torch.zeros(norm_shape, dtype=torch.int32).cuda(),torch.zeros(norm_shape, dtype=torch.int32).cuda()])\n",
    "executor.load_graph(graph=graph)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    collated_batch = {}\n",
    "    for key in batch:\n",
    "        collated_batch[key] = batch[key].to('cuda')\n",
    "    return collated_batch\n",
    "\n",
    "pipeline.optimize(\n",
    "    graph=graph, dataloader=eval_dataloader, verbose=True,\n",
    "    calib_steps=16, collate_fn=collate_fn, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ae0ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[Info] You are exporting PPQ Graph to TensorRT(Onnx + Json). \n",
      "Please Compile the TensorRT INT8 engine manually: \n",
      "\n",
      "from ppq.utils.TensorRTUtil import build_engine \n",
      "build_engine(onnx_file='Quantized.onnx', int8_scale_file='Quantized.json', engine_file='Quantized.engine', int8=True)\n",
      "\u001b[0m\n",
      "\u001b[33m[Info] Parameters have been saved to file: ./quantized.wts\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "exporter = PFL.Exporter(platform=TargetPlatform.TRT_INT8)\n",
    "exporter.export(file_path=ppq_onnxFile, graph=graph, config_path=int8_scale_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6064cb0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/01/2023-04:14:31] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/01/2023-04:14:31] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor attention_mask, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 0) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/word_embeddings/Gather_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 2) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 3) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Equal_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 9) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/Slice_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 26) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/position_embeddings/Gather_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 31) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 32) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 35) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 36) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 40) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 41) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/embeddings/LayerNorm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 43) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 44) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 48) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 49) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 51) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 52) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 54) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 55) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 61) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 62) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 64) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 65) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 67) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 68) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 101) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 102) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 114) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 115) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 117) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 122) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 123) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 125) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 126) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 131) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 132) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 135) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 136) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 140) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 141) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 143) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 144) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 146) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 147) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 149) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 150) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 152) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 153) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 156) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 157) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 160) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 161) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 163) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 164) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 166) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 167) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 172) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 173) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 176) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 177) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 181) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 182) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.0/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 184) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 185) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 189) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 190) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 192) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 193) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 195) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 196) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 202) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 203) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 205) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 206) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 208) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 209) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 242) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 243) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 255) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 256) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 258) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 263) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 264) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 266) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 267) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 272) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 273) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 276) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 277) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 281) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 282) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 284) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 285) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 287) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 288) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 290) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 291) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 293) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 294) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 297) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 298) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 301) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 302) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 304) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 305) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 307) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 308) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 313) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 314) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 317) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 318) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 322) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 323) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.1/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 325) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 326) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 330) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 331) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 333) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 334) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 336) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 337) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 343) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 344) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 346) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 347) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 349) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 350) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 383) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 384) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 396) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 397) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 399) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 404) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 405) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 407) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 408) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 413) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 414) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 417) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 418) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 422) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 423) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 425) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 426) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 428) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 429) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 431) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 432) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 434) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 435) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 438) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 439) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 442) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 443) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 445) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 446) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 448) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 449) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 454) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 455) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 458) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 459) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 463) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 464) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.2/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 466) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 467) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 471) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 472) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 474) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 475) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 477) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 478) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 484) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 485) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 487) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 488) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 490) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 491) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 524) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 525) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 537) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 538) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 540) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 545) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 546) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 548) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 549) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 554) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 555) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 558) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 559) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 563) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 564) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 566) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 567) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 569) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 570) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 572) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 573) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 575) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 576) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 579) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 580) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 583) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 584) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 586) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 587) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 589) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 590) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 595) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 596) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 599) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 600) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 604) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 605) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.3/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 607) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 608) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 612) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 613) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 615) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 616) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 618) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 619) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 625) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 626) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 628) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 629) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 631) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 632) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 665) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 666) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 678) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 679) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 681) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 686) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 687) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 689) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 690) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 695) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 696) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 699) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 700) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 704) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 705) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 707) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 708) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 710) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 711) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 713) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 714) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 716) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 717) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 720) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 721) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 724) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 725) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 727) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 728) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 730) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 731) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 736) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 737) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 740) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 741) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 745) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 746) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.4/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 748) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 749) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 753) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 754) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 756) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 757) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 759) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 760) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 766) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 767) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/q_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 769) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 770) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/k_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 772) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 773) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/v_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Reshape_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Reshape_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Reshape_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Reshape_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Transpose_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 806) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 807) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Expand_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Cast_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 819) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 820) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Where_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 822) [Softmax]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/Transpose_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 827) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 828) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 830) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 831) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/attention/out_lin/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 836) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 837) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 840) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 841) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 845) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 846) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/sa_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 848) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 849) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 851) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 852) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 854) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 855) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/lin1/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 857) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 858) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/activation/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/activation/Erf_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 861) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 862) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/activation/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/activation/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 865) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 866) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 868) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 869) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 871) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 872) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/ffn/lin2/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/Add_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/ReduceMean_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Sub_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 877) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 878) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Pow_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/ReduceMean_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 881) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 882) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Sqrt_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Div_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 886) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 887) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /distilbert/transformer/layer.5/output_layer_norm/Mul_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 889) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 890) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 892) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 893) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 895) [Constant]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor (Unnamed Layer* 896) [Shuffle]_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /qa_outputs/Add_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /Split_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor /Split_output_1, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor output_start_logits, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Missing scale and zero-point for tensor output_end_logits, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[06/01/2023-04:14:31] [TRT] [W] Detected layernorm nodes in FP16: /distilbert/embeddings/LayerNorm/Sqrt, /distilbert/embeddings/LayerNorm/ReduceMean_1, /distilbert/embeddings/LayerNorm/Sub, /distilbert/embeddings/LayerNorm/Pow, /distilbert/embeddings/LayerNorm/Add, /distilbert/embeddings/LayerNorm/Div, /distilbert/embeddings/LayerNorm/Mul, /distilbert/embeddings/LayerNorm/Add_1, /distilbert/transformer/layer.0/sa_layer_norm/Sub, /distilbert/transformer/layer.0/sa_layer_norm/Pow, /distilbert/transformer/layer.0/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.0/sa_layer_norm/Add, /distilbert/transformer/layer.0/sa_layer_norm/Sqrt, /distilbert/transformer/layer.0/sa_layer_norm/Div, /distilbert/transformer/layer.0/sa_layer_norm/Mul, /distilbert/transformer/layer.0/sa_layer_norm/Add_1, /distilbert/transformer/layer.0/output_layer_norm/Sub, /distilbert/transformer/layer.0/output_layer_norm/Pow, /distilbert/transformer/layer.0/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.0/output_layer_norm/Add, /distilbert/transformer/layer.0/output_layer_norm/Sqrt, /distilbert/transformer/layer.0/output_layer_norm/Div, /distilbert/transformer/layer.0/output_layer_norm/Mul, /distilbert/transformer/layer.0/output_layer_norm/Add_1, /distilbert/transformer/layer.1/sa_layer_norm/Sub, /distilbert/transformer/layer.1/sa_layer_norm/Pow, /distilbert/transformer/layer.1/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.1/sa_layer_norm/Add, /distilbert/transformer/layer.1/sa_layer_norm/Sqrt, /distilbert/transformer/layer.1/sa_layer_norm/Div, /distilbert/transformer/layer.1/sa_layer_norm/Mul, /distilbert/transformer/layer.1/sa_layer_norm/Add_1, /distilbert/transformer/layer.1/output_layer_norm/Sub, /distilbert/transformer/layer.1/output_layer_norm/Pow, /distilbert/transformer/layer.1/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.1/output_layer_norm/Add, /distilbert/transformer/layer.1/output_layer_norm/Sqrt, /distilbert/transformer/layer.1/output_layer_norm/Div, /distilbert/transformer/layer.1/output_layer_norm/Mul, /distilbert/transformer/layer.1/output_layer_norm/Add_1, /distilbert/transformer/layer.2/sa_layer_norm/Sub, /distilbert/transformer/layer.2/sa_layer_norm/Pow, /distilbert/transformer/layer.2/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.2/sa_layer_norm/Add, /distilbert/transformer/layer.2/sa_layer_norm/Sqrt, /distilbert/transformer/layer.2/sa_layer_norm/Div, /distilbert/transformer/layer.2/sa_layer_norm/Mul, /distilbert/transformer/layer.2/sa_layer_norm/Add_1, /distilbert/transformer/layer.2/output_layer_norm/Sub, /distilbert/transformer/layer.2/output_layer_norm/Pow, /distilbert/transformer/layer.2/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.2/output_layer_norm/Add, /distilbert/transformer/layer.2/output_layer_norm/Sqrt, /distilbert/transformer/layer.2/output_layer_norm/Div, /distilbert/transformer/layer.2/output_layer_norm/Mul, /distilbert/transformer/layer.2/output_layer_norm/Add_1, /distilbert/transformer/layer.3/sa_layer_norm/Sub, /distilbert/transformer/layer.3/sa_layer_norm/Pow, /distilbert/transformer/layer.3/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.3/sa_layer_norm/Add, /distilbert/transformer/layer.3/sa_layer_norm/Sqrt, /distilbert/transformer/layer.3/sa_layer_norm/Div, /distilbert/transformer/layer.3/sa_layer_norm/Mul, /distilbert/transformer/layer.3/sa_layer_norm/Add_1, /distilbert/transformer/layer.3/output_layer_norm/Sub, /distilbert/transformer/layer.3/output_layer_norm/Pow, /distilbert/transformer/layer.3/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.3/output_layer_norm/Add, /distilbert/transformer/layer.3/output_layer_norm/Sqrt, /distilbert/transformer/layer.3/output_layer_norm/Div, /distilbert/transformer/layer.3/output_layer_norm/Mul, /distilbert/transformer/layer.3/output_layer_norm/Add_1, /distilbert/transformer/layer.4/sa_layer_norm/Sub, /distilbert/transformer/layer.4/sa_layer_norm/Pow, /distilbert/transformer/layer.4/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.4/sa_layer_norm/Add, /distilbert/transformer/layer.4/sa_layer_norm/Sqrt, /distilbert/transformer/layer.4/sa_layer_norm/Div, /distilbert/transformer/layer.4/sa_layer_norm/Mul, /distilbert/transformer/layer.4/sa_layer_norm/Add_1, /distilbert/transformer/layer.4/output_layer_norm/Sub, /distilbert/transformer/layer.4/output_layer_norm/Pow, /distilbert/transformer/layer.4/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.4/output_layer_norm/Add, /distilbert/transformer/layer.4/output_layer_norm/Sqrt, /distilbert/transformer/layer.4/output_layer_norm/Div, /distilbert/transformer/layer.4/output_layer_norm/Mul, /distilbert/transformer/layer.4/output_layer_norm/Add_1, /distilbert/transformer/layer.5/sa_layer_norm/Sub, /distilbert/transformer/layer.5/sa_layer_norm/Pow, /distilbert/transformer/layer.5/sa_layer_norm/ReduceMean_1, /distilbert/transformer/layer.5/sa_layer_norm/Add, /distilbert/transformer/layer.5/sa_layer_norm/Sqrt, /distilbert/transformer/layer.5/sa_layer_norm/Div, /distilbert/transformer/layer.5/sa_layer_norm/Mul, /distilbert/transformer/layer.5/sa_layer_norm/Add_1, /distilbert/transformer/layer.5/output_layer_norm/Sub, /distilbert/transformer/layer.5/output_layer_norm/Pow, /distilbert/transformer/layer.5/output_layer_norm/ReduceMean_1, /distilbert/transformer/layer.5/output_layer_norm/Add, /distilbert/transformer/layer.5/output_layer_norm/Sqrt, /distilbert/transformer/layer.5/output_layer_norm/Div, /distilbert/transformer/layer.5/output_layer_norm/Mul, /distilbert/transformer/layer.5/output_layer_norm/Add_1\n",
      "[06/01/2023-04:14:31] [TRT] [W] Running layernorm after self-attention in FP16 may cause overflow. Forcing layernorm layers to run in FP32 precision can help with preserving accuracy.\n",
      "[06/01/2023-04:15:02] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/01/2023-04:15:02] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/01/2023-04:15:02] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[06/01/2023-04:15:02] [TRT] [W] - 62 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/01/2023-04:15:02] [TRT] [W] - 38 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[06/01/2023-04:15:02] [TRT] [W] - 6 weights are affected by this issue: Detected finite FP32 values which would overflow in FP16 and converted them to the closest finite FP16 value.\n"
     ]
    }
   ],
   "source": [
    "from ppq.utils.TensorRTUtil import build_engine \n",
    "build_engine(onnx_file=ppq_onnxFile, int8_scale_file=int8_scale_file, engine_file=engine_file, int8=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a00baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = trt.Logger(trt.Logger.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea7613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2298/2873862799.py:17: DeprecationWarning: Use set_input_shape instead.\n",
      "  context.set_binding_shape(i, (norm_batch_size, max_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/01/2023-04:15:03] [TRT] [I] Loaded engine size: 127 MiB\n",
      "[06/01/2023-04:15:03] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +126, now: CPU 0, GPU 126 (MiB)\n",
      "***** Engine IO *****\n",
      "input id: 0    is input:  TensorIOMode.INPUT   binding name: input_ids   shape: (16, 384) type:  DataType.INT32\n",
      "input id: 1    is input:  TensorIOMode.INPUT   binding name: attention_mask   shape: (16, 384) type:  DataType.INT32\n",
      "input id: 2    is input:  TensorIOMode.OUTPUT   binding name: output_start_logits   shape: (16, 384) type:  DataType.FLOAT\n",
      "input id: 3    is input:  TensorIOMode.OUTPUT   binding name: output_end_logits   shape: (16, 384) type:  DataType.FLOAT\n",
      "*****           *****\n",
      "[06/01/2023-04:15:03] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +234, now: CPU 0, GPU 360 (MiB)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "674it [00:11, 58.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done in total 11.625 secs (0.001 sec per example)\n",
      "Average Inference Time = 7.863 ms\n",
      "Total Inference Time =  5299.531 ms\n",
      "Total Number of Inference =  674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(engine_file, \"rb\") as f, trt.Runtime(logger) as runtime, runtime.deserialize_cuda_engine(\n",
    "    f.read()\n",
    ") as engine, engine.create_execution_context() as context:\n",
    "    input_len = 0\n",
    "    print(\"***** Engine IO *****\")\n",
    "    for idx in range(engine.num_bindings):\n",
    "        name = engine.get_tensor_name (idx)\n",
    "        is_input = engine.get_tensor_mode (name)\n",
    "        if is_input == trt.TensorIOMode.INPUT:\n",
    "            input_len += 1\n",
    "        op_type = engine.get_tensor_dtype(name)\n",
    "        shape = engine.get_tensor_shape(name)\n",
    "        print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)\n",
    "    print(\"*****           *****\")\n",
    "        \n",
    "    for i in range(input_len):\n",
    "        context.set_binding_shape(i, (norm_batch_size, max_length))\n",
    "    assert context.all_binding_shapes_specified\n",
    "    \n",
    "    inputs, outputs, bindings, stream = common.allocate_buffers(engine, context, 0)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"***** Running Evaluation *****\")\n",
    "    print(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    print(f\"  Batch size = {norm_batch_size}\")\n",
    "\n",
    "    total_time = 0.0\n",
    "    niter = 0\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    all_preds = None\n",
    "        \n",
    "    for step, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        input_ids = np.asarray(batch[\"input_ids\"], dtype=np.int32)\n",
    "        attention_mask = np.asarray(batch[\"attention_mask\"], dtype=np.int32)\n",
    "\n",
    "        inputs[0].host = input_ids.ravel()\n",
    "        inputs[1].host = attention_mask.ravel()\n",
    "        \n",
    "        trt_outputs, infer_time = common.do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "\n",
    "        start_logits, end_logits = trt_outputs\n",
    "        start_logits = torch.tensor(start_logits).reshape(norm_batch_size, max_length)\n",
    "        end_logits = torch.tensor(end_logits).reshape(norm_batch_size, max_length)\n",
    "        \n",
    "        total_time += infer_time\n",
    "        niter += 1\n",
    "\n",
    "        # necessary to pad predictions and labels for being gathered\n",
    "        start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "        end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        logits = (accelerator.gather(start_logits).cpu().numpy(), accelerator.gather(end_logits).cpu().numpy())\n",
    "        all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n",
    "\n",
    "    if all_preds is not None:\n",
    "        all_preds = nested_truncate(all_preds, len(eval_dataset))\n",
    "        \n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    print(f\"Evaluation done in total {evalTime:.3f} secs ({evalTime / len(eval_dataset):.3f} sec per example)\")\n",
    "    # Inference time from TRT\n",
    "    print(\"Average Inference Time = {:.3f} ms\".format(total_time * 1000 / niter))\n",
    "    print(\"Total Inference Time =  {:.3f} ms\".format(total_time * 1000))\n",
    "    print(f\"Total Number of Inference =  {niter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa53805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be288da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 10570/10570 [00:26<00:00, 398.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'exact_match': 74.12488174077578, 'f1': 84.28104923286605}\n"
     ]
    }
   ],
   "source": [
    "squad_v2 = False\n",
    "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, all_preds)\n",
    "\n",
    "eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "print(f\"Evaluation metrics: {eval_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08493b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
