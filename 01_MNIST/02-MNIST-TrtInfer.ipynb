{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from cuda import cuda, cudart\n",
    "from typing import Optional, List\n",
    "import ctypes\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5df021",
   "metadata": {},
   "source": [
    "## 1. 建立 Logger (日志记录器)\n",
    "\n",
    "可选参数：`Logger.VERBOSE`、`Logger.INFO`、`Logger.WARNING`、`Logger.ERROR`、`Logger.INTERNAL_ERROR`、`Logger.UNKNOWN` 对应日志输出的详细或严重程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd7c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = trt.Logger(trt.Logger.ERROR) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88735ec",
   "metadata": {},
   "source": [
    "## 2. 建立 Builder 和 BuilderConfig\n",
    "\n",
    "Builder 类是 TensorRT 中的主要类之一，用于构建和优化 TensorRT 引擎。在创建 Builder 对象时，需要传入一个 Logger 对象，用于记录日志和错误信息。Builder 可以使用 BuilderConfig 对象来设置一些元数据，如最大批处理大小、最大 workspace 大小等等。\n",
    "\n",
    "BuilderConfig 类是用于设置构建 TensorRT 引擎时的一些元数据的类。它可以设置一些参数，例如最大批处理大小、最大 workspace 大小、最大延迟等等。BuilderConfig 还可以设置 TensorRT 引擎的精度（FP32、FP16 或 INT8）和优化方式（如批处理大小、数据类型等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3408ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = trt.Builder(logger)                                           # create Builder\n",
    "config = builder.create_builder_config()                                # create BuidlerConfig to set meta data of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afb33d",
   "metadata": {},
   "source": [
    "## 3. 创建 Network\n",
    "\n",
    "Network 的构建主要分为两种方法：\n",
    "\n",
    "使用 Parser (TF/Torch -> ONNX -> TensorRT)\n",
    "* 流程成熟，ONNX 通用性好，方便网络调整，兼顾效率性能\n",
    "\n",
    "使用 TensorRT 原生 API 搭建网络\n",
    "* 性能最优，精细网络控制，兼容性最好\n",
    "\n",
    "由于使用 API 搭建网络开发效率较低，较复杂，所以一般采用使用 Parser 的方式搭建网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f4acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded parsing .onnx file!\n"
     ]
    }
   ],
   "source": [
    "onnxFile = \"./model.onnx\"\n",
    "nHeight = 28\n",
    "nWidth = 28\n",
    "\n",
    "# 创建 Network 使用 Explicit Batch 模式，所有的维度都是显式的并且是动态的，意思是在执行的时候，每一维度的长度都可以变化\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "\n",
    "# 对 ONNX 进行模型解析\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "with open(onnxFile, \"rb\") as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        print(\"Failed parsing .onnx file!\")\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "        exit()\n",
    "    print(\"Succeeded parsing .onnx file!\")\n",
    "\n",
    "# 由于使用 dynamic shape 需要 profile 指定输入范围，并让 profile 优化不同 shape 对应不同的 kernel\n",
    "profile = builder.create_optimization_profile()\n",
    "\n",
    "# 获取网络的输入张量\n",
    "inputTensor = network.get_input(0)\n",
    "\n",
    "# 设置优化配置文件中输入张量的形状，包括最小、最优和最大形状\n",
    "profile.set_shape(inputTensor.name, [1, 1, nHeight, nWidth], [4, 1, nHeight, nWidth], [8, 1, nHeight, nWidth])\n",
    "\n",
    "# 将优化配置文件添加到TensorRT配置中\n",
    "config.add_optimization_profile(profile) \n",
    "\n",
    "# 移除输出张量 \"y\" 的标记 原 network.get_output(0).name == 'y'\n",
    "network.unmark_output(network.get_output(0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62f51f",
   "metadata": {},
   "source": [
    "---\n",
    "1. 使用build_engine函数构建了一个TensorRT引擎（engine）。引擎（engine）是一个已编译和优化的网络表示形式。引擎通常是存储在内存中的对象，可以直接用于推理过程，而无需重新构建和优化网络。引擎还可以通过将其序列化为一个计划（plan）进行持久化。\n",
    "\n",
    "2. 使用build_serialized_network函数构建了一个可序列化的计划（plan）。一个计划（plan）是一个包含了网络结构和权重信息的序列化二进制对象。该计划可以保存到磁盘中，以便在以后的时间加载和重用。\n",
    "\n",
    "总结来说，计划（plan）是一种可序列化的网络表示形式，可以保存到磁盘并在需要时加载和重用。引擎（engine）是一个已编译和优化的网络对象，可以直接用于推理过程，无需重新构建和优化网络。通常，你可以通过构建引擎（engine）来进行实时推理，而使用计划（plan）可以在不同的时间和设备上加载和重用网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab1871",
   "metadata": {},
   "source": [
    "## (4.直接生成 Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf79b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10745/4271150106.py:1: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    }
   ],
   "source": [
    "engine = builder.build_engine(network, config)\n",
    "with open('model.engine', \"wb\") as f:\n",
    "    f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02160cf",
   "metadata": {},
   "source": [
    "## 4. 生成网络的 TRT 内部表示 serialized network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb72d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded building serialized engine!\n",
      "Succeeded saving .plan file!\n"
     ]
    }
   ],
   "source": [
    "trtFile = \"model.plan\"\n",
    "\n",
    "# create a serialized network\n",
    "engineString = builder.build_serialized_network(network, config)        \n",
    "if engineString == None:\n",
    "    print(\"Failed building serialized engine!\")\n",
    "print(\"Succeeded building serialized engine!\")\n",
    "\n",
    "# write the serialized netwok into a .plan file\n",
    "with open(trtFile, \"wb\") as f:                                          \n",
    "    f.write(engineString)\n",
    "    print(\"Succeeded saving .plan file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a8d50",
   "metadata": {},
   "source": [
    "## 5. 生成 Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97c1268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded building engine!\n"
     ]
    }
   ],
   "source": [
    "# trtFile = \"model.plan\"\n",
    "# if os.path.isfile(trtFile):                                                 # load serialized network and skip building process if .plan file existed\n",
    "#     with open(trtFile, \"rb\") as f:\n",
    "#         engineString = f.read()\n",
    "#     if engineString == None:\n",
    "#         print(\"Failed getting serialized engine!\")\n",
    "#     print(\"Succeeded getting serialized engine!\")\n",
    "\n",
    "engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "\n",
    "if engine == None:\n",
    "    print(\"Failed building engine!\")\n",
    "print(\"Succeeded building engine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db593b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id: 0    is input:  TensorIOMode.INPUT   binding name: x   shape: (-1, 1, 28, 28) type:  DataType.FLOAT\n",
      "input id: 1    is input:  TensorIOMode.OUTPUT   binding name: z   shape: (-1,) type:  DataType.INT32\n"
     ]
    }
   ],
   "source": [
    "for idx in range(engine.num_bindings):\n",
    "    \n",
    "    name = engine.get_tensor_name (idx)\n",
    "    is_input = engine.get_tensor_mode (name)\n",
    "    op_type = engine.get_tensor_dtype(name)\n",
    "    shape = engine.get_tensor_shape(name)\n",
    "\n",
    "    print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb86ca",
   "metadata": {},
   "source": [
    "## 6. 创建 buffers\n",
    "用于申请 inputs / outputs 的显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5069da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocates all buffers required for an engine, i.e. host/device inputs/outputs.\n",
    "# If engine uses dynamic shapes, specify a profile to find the maximum input & output size.\n",
    "\n",
    "def check_cuda_err(err):\n",
    "    if isinstance(err, cuda.CUresult):\n",
    "        if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "            raise RuntimeError(\"Cuda Error: {}\".format(err))\n",
    "    if isinstance(err, cudart.cudaError_t):\n",
    "        if err != cudart.cudaError_t.cudaSuccess:\n",
    "            raise RuntimeError(\"Cuda Runtime Error: {}\".format(err))\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown error type: {}\".format(err))\n",
    "\n",
    "def cuda_call(call):\n",
    "    err, res = call[0], call[1:]\n",
    "    check_cuda_err(err)\n",
    "    if len(res) == 1:\n",
    "        res = res[0]\n",
    "    return res\n",
    "\n",
    "def load_test_case(pagelocked_buffer, img):\n",
    "    copy_size = img.ravel().size\n",
    "    np.copyto(pagelocked_buffer[:int(copy_size)], img.ravel())\n",
    "    \n",
    "class HostDeviceMem:\n",
    "    \"\"\"Pair of host and device memory, where the host memory is wrapped in a numpy array\"\"\"\n",
    "    def __init__(self, size: int, dtype: np.dtype):\n",
    "        nbytes = size * dtype.itemsize\n",
    "        host_mem = cuda_call(cudart.cudaMallocHost(nbytes))\n",
    "        pointer_type = ctypes.POINTER(np.ctypeslib.as_ctypes_type(dtype))\n",
    "\n",
    "        self._host = np.ctypeslib.as_array(ctypes.cast(host_mem, pointer_type), (size,))\n",
    "        self._device = cuda_call(cudart.cudaMalloc(nbytes))\n",
    "        self._nbytes = nbytes\n",
    "\n",
    "    @property\n",
    "    def host(self) -> np.ndarray:\n",
    "        return self._host\n",
    "\n",
    "    @host.setter\n",
    "    def host(self, arr: np.ndarray):\n",
    "        if arr.size > self.host.size:\n",
    "            raise ValueError(\n",
    "                f\"Tried to fit an array of size {arr.size} into host memory of size {self.host.size}\"\n",
    "            )\n",
    "        np.copyto(self.host[:arr.size], arr.flat, casting='safe')\n",
    "\n",
    "    @property\n",
    "    def device(self) -> int:\n",
    "        return self._device\n",
    "\n",
    "    @property\n",
    "    def nbytes(self) -> int:\n",
    "        return self._nbytes\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Host:\\n{self.host}\\nDevice:\\n{self.device}\\nSize:\\n{self.nbytes}\\n\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def free(self):\n",
    "        cuda_call(cudart.cudaFree(self.device))\n",
    "        cuda_call(cudart.cudaFreeHost(self.host.ctypes.data))\n",
    "        \n",
    "def allocate_buffers(engine: trt.ICudaEngine, profile_idx: Optional[int] = None, shape_idx = -1):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    \n",
    "    batch_size = 0\n",
    "    \n",
    "    stream = cuda_call(cudart.cudaStreamCreate())\n",
    "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
    "    \n",
    "    for binding in tensor_names:\n",
    "        # get_tensor_profile_shape returns (min_shape, optimal_shape, max_shape)\n",
    "        # Pick out the max shape to allocate enough memory for the binding.\n",
    "        shape = engine.get_tensor_shape(binding) if profile_idx is None else engine.get_tensor_profile_shape(binding, profile_idx)[shape_idx]\n",
    "        if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "            batch_size = shape[0]\n",
    "        if engine.get_tensor_mode(binding) == trt.TensorIOMode.OUTPUT:\n",
    "            shape = (batch_size,)\n",
    "        # print(\"shape\", binding, shape)\n",
    "        \n",
    "        shape_valid = np.all([s >= 0 for s in shape])\n",
    "        if not shape_valid and profile_idx is None:\n",
    "            raise ValueError(f\"Binding {binding} has dynamic shape, \" +\\\n",
    "                \"but no profile was specified.\")\n",
    "        size = trt.volume(shape)\n",
    "        if engine.has_implicit_batch_dimension:\n",
    "            size *= engine.max_batch_size\n",
    "        dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(binding)))\n",
    "\n",
    "        # print(\"size\", size)\n",
    "        # Allocate host and device buffers\n",
    "        bindingMemory = HostDeviceMem(size, dtype)\n",
    "\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(bindingMemory.device))\n",
    "\n",
    "        # Append to the appropriate list.\n",
    "        if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "            inputs.append(bindingMemory)\n",
    "        else:\n",
    "            outputs.append(bindingMemory)\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def free_buffers(inputs: List[HostDeviceMem], outputs: List[HostDeviceMem], stream: cudart.cudaStream_t):\n",
    "    for mem in inputs + outputs:\n",
    "        mem.free()\n",
    "    cuda_call(cudart.cudaStreamDestroy(stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2d8b0",
   "metadata": {},
   "source": [
    "## 7. 推理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e522912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_inference_base(inputs, outputs, stream, execute_async):\n",
    "    # Transfer input data to the GPU.\n",
    "    kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n",
    "    [cuda_call(cudart.cudaMemcpyAsync(inp.device, inp.host, inp.nbytes, kind, stream)) for inp in inputs]\n",
    "    # Run inference.\n",
    "    execute_async()\n",
    "    # Transfer predictions back from the GPU.\n",
    "    kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost\n",
    "    [cuda_call(cudart.cudaMemcpyAsync(out.host, out.device, out.nbytes, kind, stream)) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    cuda_call(cudart.cudaStreamSynchronize(stream))\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    def execute_async():\n",
    "        context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream)\n",
    "    return _do_inference_base(inputs, outputs, stream, execute_async)\n",
    "\n",
    "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
    "    def execute_async():\n",
    "        context.execute_async_v2(bindings=bindings, stream_handle=stream)\n",
    "    return _do_inference_base(inputs, outputs, stream, execute_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597504b",
   "metadata": {},
   "source": [
    "## 测试-对单张图片进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c43879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadMnistData import MyData\n",
    "import torch\n",
    "\n",
    "DATA_PATH = \"../data/MNIST/\"\n",
    "\n",
    "testDataset = MyData(datapath = DATA_PATH, isTrain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77ee946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_1 = testDataset[12][0] # torch.Size([1, 28, 28])\n",
    "# 将 Torch 张量转换为 NumPy 数组\n",
    "img_np = img_1.squeeze().numpy()\n",
    "\n",
    "# 使用 Matplotlib 显示图像\n",
    "plt.imshow(img_np, cmap='gray')\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9600fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 9\n"
     ]
    }
   ],
   "source": [
    "with engine.create_execution_context() as context:\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine, 0)\n",
    "    context.set_optimization_profile_async(0, stream)\n",
    "    \n",
    "    data = img_1.unsqueeze(0).numpy()\n",
    "    context.set_input_shape('x', data.shape)\n",
    "    \n",
    "    inputs[0].host = data\n",
    "    # load_test_case(inputs[0].host, data)\n",
    "    \n",
    "    trt_outputs = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "    # result = trt_outputs\n",
    "    print(\"result\", trt_outputs[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7908116",
   "metadata": {},
   "outputs": [],
   "source": [
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc0f4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_buffers(inputs, outputs, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc5598",
   "metadata": {},
   "source": [
    "## 测试-模型精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865f3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadMnistData import MyData\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "DATA_PATH = \"../data/MNIST/\"\n",
    "\n",
    "testDataset = MyData(datapath = DATA_PATH, isTrain = False)\n",
    "testLoader = torch.utils.data.DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8cd222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs, bindings, stream = allocate_buffers(engine, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4b75123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tensorrt_acc(engine, test_loader):\n",
    "    correct = 0\n",
    "    with engine.create_execution_context() as context:\n",
    "        context.set_optimization_profile_async(0, stream)\n",
    "        for _, (image, label) in enumerate(test_loader):\n",
    "            image = image.numpy()\n",
    "            context.set_input_shape('x', image.shape)\n",
    "            inputs[0].host = image\n",
    "            # load_test_case(inputs[0].host, image)\n",
    "            preds = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)[0]\n",
    "            label = np.argmax(label.numpy(), axis=-1)\n",
    "            count = 0\n",
    "            for i in range(len(label)):\n",
    "                if label[i] == preds[i]:\n",
    "                    count += 1\n",
    "            correct += count\n",
    "        del context\n",
    "    print('\\nTest set: Accuracy: {:.3f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8906bb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 96.300%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tensorrt_acc(engine, testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aadc276",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_buffers(inputs, outputs, stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a76f9f",
   "metadata": {},
   "source": [
    "## 测试-推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52b648a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs, bindings, stream = allocate_buffers(engine, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4528840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test_tensorrt_for_test(engine):\n",
    "    i = 0\n",
    "    total_time_span = 0\n",
    "    with engine.create_execution_context() as context:\n",
    "        context.set_optimization_profile_async(0, stream)\n",
    "        # warm up\n",
    "        input_shape = engine.get_tensor_shape('x')\n",
    "        input_shape[0] = engine.get_tensor_profile_shape('x', 0)[-1][0]\n",
    "        print('input_shape', input_shape)\n",
    "        \n",
    "        data = np.random.rand(*input_shape).astype(np.float32)\n",
    "        \n",
    "        context.set_input_shape('x', data.shape)\n",
    "        # inputs[0].host = data\n",
    "        load_test_case(inputs[0].host, data)\n",
    "        for i in range(10):\n",
    "            pred = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        for i in range(1000):\n",
    "#             data = np.random.rand(*input_shape).astype(np.float32)\n",
    "#             load_test_case(inputs[0].host, data)\n",
    "            # =======================================\n",
    "            # The common do_inference function will return a list of outputs - we only have one in this case.\n",
    "\n",
    "            start_time = time.time()\n",
    "            pred = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "            time_span = time.time() - start_time\n",
    "\n",
    "            total_time_span += time_span\n",
    "        total_time_span /= 1000.0\n",
    "        print('total_time_span', total_time_span)\n",
    "        # del context if not reuse\n",
    "        del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "759b7bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (8, 1, 28, 28)\n",
      "total_time_span 0.00010671162605285645\n"
     ]
    }
   ],
   "source": [
    "test_tensorrt_for_test(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f0f000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_buffers(inputs, outputs, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1fd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba489c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 构造模型\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, (5, 5), padding=(2, 2), bias=True)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, (5, 5), padding=(2, 2), bias=True)\n",
    "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 1024, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(1024, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.reshape(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        y = self.fc2(x)\n",
    "        z = F.softmax(y, dim=1)\n",
    "        z = torch.argmax(z, dim=1)\n",
    "        return y, z\n",
    "    \n",
    "model = Net().cuda()\n",
    "\n",
    "# def test_tensorrt_for_test(engine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c6f1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch_for_test(engine):\n",
    "    i = 0\n",
    "    total_time_span = 0\n",
    "    # warm up\n",
    "    input_shape = engine.get_tensor_shape('x')\n",
    "    input_shape[0] = engine.get_tensor_profile_shape('x', 0)[-1][0]\n",
    "    print('input_shape', input_shape)\n",
    "\n",
    "    data = torch.rand(*input_shape).cuda()\n",
    "    # load_test_case(inputs[0].host, data)\n",
    "    for i in range(10):\n",
    "        model(data)\n",
    "    for i in range(100):\n",
    "#             data = np.random.rand(*input_shape).astype(np.float32)\n",
    "#             load_test_case(inputs[0].host, data)\n",
    "        # =======================================\n",
    "        # The common do_inference function will return a list of outputs - we only have one in this case.\n",
    "\n",
    "        start_time = time.time()\n",
    "        model(data)\n",
    "        time_span = time.time() - start_time\n",
    "\n",
    "        total_time_span += time_span\n",
    "    total_time_span /= 100.0\n",
    "    print('total_time_span', total_time_span)\n",
    "    # del context if not reuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e8d4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (8, 1, 28, 28)\n",
      "total_time_span 0.0003108072280883789\n"
     ]
    }
   ],
   "source": [
    "test_torch_for_test(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bca5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
